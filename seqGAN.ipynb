{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrSsOv3uBsuB"
   },
   "source": [
    "## Install Requirements and Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVMevIgBzja"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!wget http://mtg.upf.edu/static/datasets/last.fm/lastfm-dataset-1K.tar.gz\n",
    "!tar -xzf lastfm-dataset-1K.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6OkyMHoNGmW9"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-EHgOL_Go2L"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bno-JbOdDRQ3"
   },
   "source": [
    "## Read In Last.fm Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-eH8Jk4DV3e"
   },
   "outputs": [],
   "source": [
    "def read_data(nRows=None):\n",
    "    start_time = time.time()\n",
    "    directory = \"lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\"\n",
    "    df = pd.read_csv(directory, engine='python',\n",
    "                          nrows=nRows, header=None, sep='\\\\t',\n",
    "                          names=['userId', 'date', 'artist_id', 'artist_name', 'track_id', 'track_name'])\n",
    "    df.dataframeName = 'userid-timestamp-artid-artname-traid-traname.tsv'\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "    nRow, nCol = df.shape\n",
    "    print(f'There are {nRow} rows and {nCol} columns')\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fP3jqLwRDMop"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Egzi-rfnCJn0"
   },
   "outputs": [],
   "source": [
    "def summary_stats(df):\n",
    "    nan_values = df.track_id.isna().sum()\n",
    "    unique_tracks = len(df.track_name.unique())\n",
    "    print(\"There are {} NaN track_ids and {} unique tracks\".format(nan_values, unique_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPeluoOEGVx2"
   },
   "outputs": [],
   "source": [
    "def find_repeated_tracks(session_frame):\n",
    "    repeats = {}\n",
    "\n",
    "    for row in session_frame.iloc[1]:\n",
    "        print(row)\n",
    "        i = 0\n",
    "\n",
    "        while i < (len(row) - 1):\n",
    "            if row[i] == row[i + 1]:\n",
    "                if 2 in repeats:\n",
    "                    repeats[2] += 1\n",
    "                else:\n",
    "                    repeats[2] = 1\n",
    "                j = i + 1\n",
    "                count = 3\n",
    "                while j < len(row) - 1:\n",
    "                    if row[j] == row[j + 1]:\n",
    "                        if count in repeats:\n",
    "                            repeats[count] += 1\n",
    "                        else:\n",
    "                            repeats[count] = 1\n",
    "                        j += 1\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        i = j\n",
    "                        break\n",
    "            i += 1\n",
    "    return repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxQVQGoRGckg"
   },
   "outputs": [],
   "source": [
    "def print_top_tracks(all_tracks, nTracks=10):\n",
    "    sorted_tracks = [(k, (all_tracks[k]['artist'], all_tracks[k]['track_name'], all_tracks[k]['plays'])) for k in\n",
    "                     sorted(all_tracks, key=lambda x: all_tracks[x]['plays'], reverse=True)]\n",
    "\n",
    "    n_items = list(islice(sorted_tracks, 10))\n",
    "\n",
    "    print(\"\\nTOP TRACKS\")\n",
    "    print(\"{:<5s} {:<40s} {:<40s} {:<6s}\".format(\"Rank\",\"Artist\",\"Track\",\"Plays\"))\n",
    "\n",
    "    for idx, tk in enumerate(n_items):\n",
    "        print(\"{:<5d} {:<40s} {:<40s} {:<6d} \".format(idx + 1, tk[1][0], tk[1][1], tk[1][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BGRwHW0HD26k"
   },
   "source": [
    "## Assign Unique track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4ohQ7T1D7k0"
   },
   "outputs": [],
   "source": [
    "def assign_unique_ids(df1):\n",
    "    print(\"Assigning unique track ids...\")\n",
    "    unique_track_id = {}\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, value in enumerate(df1.track_name.unique()):\n",
    "        unique_track_id[value] = idx\n",
    "\n",
    "    print(len(unique_track_id))\n",
    "    print(\"Completed in {:0.2f} seconds\".format(time.time() - start_time))\n",
    "    \n",
    "    return unique_track_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3SsKQQwEDkr"
   },
   "source": [
    "## Get Sessions from Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkG4k6aAEJ3W"
   },
   "outputs": [],
   "source": [
    "def sessions_from_tracks(daniel_track_id, tracks, df, session_size):\n",
    "    count=0\n",
    "    track_list = []\n",
    "    track_times = []\n",
    "\n",
    "    sessions = pd.DataFrame(columns=['user_session_ID', 'session', 'start_time', 'length'])\n",
    "    \n",
    "    session_size = np.timedelta64(session_size, 'm') \n",
    "    prev_time = np.timedelta64(0, 'us')\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        t_id = daniel_track_id[row.track_name]\n",
    "  #  Calculate track plays       \n",
    "        if t_id not in tracks:\n",
    "            tracks[t_id] = {'track_name': row.track_name,  'artist': row.artist_name, 'plays': 1}\n",
    "        else:\n",
    "            tracks[t_id]['plays'] += 1\n",
    "        track_list.append(daniel_track_id[row.track_name])\n",
    "        track_times.append(row.date)\n",
    "        \n",
    "        \n",
    "        if((row.date - prev_time)> session_size):\n",
    "            userID = (row.userId + '_session_' + str(count+1))\n",
    "            sessions = sessions.append({'user_session_ID': userID, 'session': track_list, 'start_time': track_times[0], 'length': (row.date - track_times[0])}, ignore_index=True)\n",
    "            count += 1\n",
    "            track_list = []\n",
    "            track_times = []\n",
    "        \n",
    "        prev_time = row.date\n",
    "    \n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G72ZreZYELbz"
   },
   "source": [
    "## Get Sessions per User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POelJAj7E7xM"
   },
   "outputs": [],
   "source": [
    "def sessions_from_frame(df1, daniel_track_id, session_size):\n",
    "    start_time = st = time.time()\n",
    "    all_tracks = {}\n",
    "    session_data = pd.DataFrame()\n",
    "    session_frame = pd.DataFrame(columns=['user_session_ID', 'session', 'start_time', 'length'])\n",
    "\n",
    "    for idx, group in df1.groupby('userId')[['date', 'track_id', 'track_name', 'artist_name']]:\n",
    "        group = group.sort_values(by='date', ascending=True)\n",
    "        group.date = pd.to_timedelta(group.date.astype('int64'), unit='ns')\n",
    "        session_data = sessions_from_tracks(daniel_track_id, all_tracks, group, session_size)\n",
    "        session_frame = session_frame.append(session_data, ignore_index=True)\n",
    "\n",
    "        start_time=time.time()\n",
    "      \n",
    "    return session_frame, all_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzscC9wxFoZ_"
   },
   "source": [
    "## Get Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pUzxJ8rHPLv"
   },
   "outputs": [],
   "source": [
    "def get_sessions(nRows):\n",
    "\n",
    "    df = read_data(nRows)\n",
    "    summary_stats(df)\n",
    "    unique_ids = assign_unique_ids(df)\n",
    "    session_frame, all_tracks = sessions_from_frame(df, unique_ids, session_size=20)\n",
    "#     print_top_tracks(all_tracks)\n",
    "    df_new = session_frame.session\n",
    "    \n",
    "    return df_new, all_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iiCdygQHkkR"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjZgAdonHmCy"
   },
   "outputs": [],
   "source": [
    "def _cumsum(x, length):\n",
    "    lower_triangular_ones = tf.constant(\n",
    "        np.tril(np.ones((length, length))),\n",
    "        dtype=tf.float32)\n",
    "    return tf.reshape(\n",
    "            tf.matmul(lower_triangular_ones,\n",
    "                      tf.reshape(x, [length, 1])),\n",
    "            [length])\n",
    "\n",
    "\n",
    "def _backwards_cumsum(x, length):\n",
    "    upper_triangular_ones = tf.constant(\n",
    "        np.triu(np.ones((length, length))),\n",
    "        dtype=tf.float32)\n",
    "    return tf.reshape(\n",
    "            tf.matmul(upper_triangular_ones,\n",
    "                      tf.reshape(x, [length, 1])),\n",
    "            [length])\n",
    "\n",
    "\n",
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim,\n",
    "                 sequence_length, start_token,\n",
    "                 learning_rate=0.01, reward_gamma=0.9):\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = tf.constant(start_token, dtype=tf.int32)\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.reward_gamma = reward_gamma\n",
    "        self.g_params = []\n",
    "        self.d_params = []\n",
    "\n",
    "        self.expected_reward = tf.Variable(tf.zeros([self.sequence_length]))\n",
    "\n",
    "        with tf.compat.v1.variable_scope('generator'):\n",
    "            self.g_embeddings = tf.Variable(self.init_matrix([self.num_emb, self.emb_dim]))\n",
    "            self.g_params.append(self.g_embeddings)\n",
    "            self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator\n",
    "            self.g_output_unit = self.create_output_unit(self.g_params, self.g_embeddings)  # maps h_t to o_t (output token logits)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('discriminator'):\n",
    "            self.d_embeddings = tf.Variable(self.init_matrix([self.num_emb, self.emb_dim]))\n",
    "            self.d_params.append(self.d_embeddings)\n",
    "            self.d_recurrent_unit = self.create_recurrent_unit(self.d_params)  # maps h_tm1 to h_t for discriminator\n",
    "            self.d_classifier_unit = self.create_classifier_unit(self.d_params)  # maps h_t to class prediction logits\n",
    "            self.d_h0 = tf.Variable(self.init_vector([self.hidden_dim]))\n",
    "            self.d_params.append(self.d_h0)\n",
    "\n",
    "        self.h0 = tf.compat.v1.placeholder(tf.float32, shape=[self.hidden_dim])  # initial random vector for generator\n",
    "        self.x = tf.compat.v1.placeholder(tf.int32, shape=[self.sequence_length])  # sequence of indices of true data, not including start token\n",
    "        self.samples = tf.compat.v1.placeholder(tf.float32, shape=[self.sequence_length])  # random samples from [0, 1]\n",
    "\n",
    "        # generator on initial randomness\n",
    "        gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "        samples = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length)\n",
    "        samples = samples.unstack(self.samples)\n",
    "        def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x):\n",
    "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
    "            o_t = self.g_output_unit(h_t)\n",
    "            sample = samples.read(i)\n",
    "            o_cumsum = _cumsum(o_t, self.num_emb)  # prepare for sampling ***\n",
    "            next_token = tf.compat.v1.to_int32(tf.reduce_min(tf.where(sample < o_cumsum)))  # sample\n",
    "            x_tp1 = tf.gather(self.g_embeddings, next_token)\n",
    "            gen_o = gen_o.write(i, tf.gather(o_t, next_token))  # we only need the sampled token's probability\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, not embeddings\n",
    "            return i + 1, x_tp1, h_t, gen_o, gen_x\n",
    "\n",
    "        _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
    "            body=_g_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.gather(self.g_embeddings, self.start_token),\n",
    "                       self.h0, gen_o, gen_x))\n",
    "\n",
    "        # discriminator on generated and real data\n",
    "        d_gen_predictions = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length,\n",
    "            dynamic_size=False, infer_shape=True)\n",
    "        d_real_predictions = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length,\n",
    "            dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        self.gen_x = self.gen_x.stack()\n",
    "        emb_gen_x = tf.gather(self.d_embeddings, self.gen_x)\n",
    "        ta_emb_gen_x = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_gen_x = ta_emb_gen_x.unstack(emb_gen_x)\n",
    "\n",
    "        emb_real_x = tf.gather(self.d_embeddings, self.x)\n",
    "        ta_emb_real_x = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_real_x = ta_emb_real_x.unstack(emb_real_x)\n",
    "\n",
    "        def _d_recurrence(i, inputs, h_tm1, pred):\n",
    "            x_t = inputs.read(i)\n",
    "            h_t = self.d_recurrent_unit(x_t, h_tm1)\n",
    "            y_t = self.d_classifier_unit(h_t)\n",
    "            pred = pred.write(i, y_t)\n",
    "            return i + 1, inputs, h_t, pred\n",
    "\n",
    "        _, _, _, self.d_gen_predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_d_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       ta_emb_gen_x,\n",
    "                       self.d_h0,\n",
    "                       d_gen_predictions))\n",
    "        self.d_gen_predictions = tf.reshape(\n",
    "                self.d_gen_predictions.stack(),\n",
    "                [self.sequence_length])\n",
    "\n",
    "        _, _, _, self.d_real_predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_d_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       ta_emb_real_x,\n",
    "                       self.d_h0,\n",
    "                       d_real_predictions))\n",
    "        self.d_real_predictions = tf.reshape(\n",
    "                self.d_real_predictions.stack(),\n",
    "                [self.sequence_length])\n",
    "\n",
    "        # supervised pretraining for generator\n",
    "        g_predictions = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length,\n",
    "            dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        emb_x = tf.gather(self.g_embeddings, self.x)\n",
    "        ta_emb_x = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_x = ta_emb_x.unstack(emb_x)\n",
    "\n",
    "        def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
    "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
    "            o_t = self.g_output_unit(h_t)\n",
    "            g_predictions = g_predictions.write(i, o_t)\n",
    "            x_tp1 = ta_emb_x.read(i)\n",
    "            return i + 1, x_tp1, h_t, g_predictions\n",
    "\n",
    "        _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_pretrain_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.gather(self.g_embeddings, self.start_token),\n",
    "                       self.h0, g_predictions))\n",
    "\n",
    "        self.g_predictions = tf.reshape(\n",
    "                self.g_predictions.stack(),\n",
    "                [self.sequence_length, self.num_emb])\n",
    "\n",
    "        # calculate discriminator loss\n",
    "        self.d_gen_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=self.d_gen_predictions, labels=tf.zeros([self.sequence_length])))\n",
    "        self.d_real_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=self.d_real_predictions, labels=tf.ones([self.sequence_length])))\n",
    "\n",
    "        # calculate generator rewards and loss\n",
    "        decays = tf.exp(tf.math.log(self.reward_gamma) * tf.compat.v1.to_float(tf.range(self.sequence_length)))\n",
    "        rewards = _backwards_cumsum(decays * tf.sigmoid(self.d_gen_predictions),\n",
    "                                    self.sequence_length)\n",
    "        normalized_rewards = \\\n",
    "            rewards / _backwards_cumsum(decays, self.sequence_length) - self.expected_reward\n",
    "\n",
    "        self.reward_loss = tf.reduce_mean(normalized_rewards ** 2)\n",
    "        self.g_loss = \\\n",
    "            -tf.reduce_mean(tf.math.log(self.gen_o.stack()) * normalized_rewards)\n",
    "\n",
    "        # pretraining loss\n",
    "        self.pretrain_loss = \\\n",
    "            (-tf.reduce_sum(\n",
    "                tf.one_hot(tf.compat.v1.to_int64(self.x),\n",
    "                           self.num_emb, 1.0, 0.0) * tf.math.log(self.g_predictions))\n",
    "             / self.sequence_length)\n",
    "\n",
    "        # training updates\n",
    "        d_opt = self.d_optimizer(self.learning_rate)\n",
    "        g_opt = self.g_optimizer(self.learning_rate)\n",
    "        pretrain_opt = self.g_optimizer(self.learning_rate)\n",
    "        reward_opt = tf.compat.v1.train.GradientDescentOptimizer(self.learning_rate)\n",
    "\n",
    "        self.d_gen_grad = tf.gradients(self.d_gen_loss, self.d_params)\n",
    "        self.d_real_grad = tf.gradients(self.d_real_loss, self.d_params)\n",
    "        self.d_gen_updates = d_opt.apply_gradients(zip(self.d_gen_grad, self.d_params))\n",
    "        self.d_real_updates = d_opt.apply_gradients(zip(self.d_real_grad, self.d_params))\n",
    "\n",
    "        self.reward_grad = tf.gradients(self.reward_loss, [self.expected_reward])\n",
    "        self.reward_updates = reward_opt.apply_gradients(zip(self.reward_grad, [self.expected_reward]))\n",
    "\n",
    "        self.g_grad = tf.gradients(self.g_loss, self.g_params)\n",
    "        self.g_updates = g_opt.apply_gradients(zip(self.g_grad, self.g_params))\n",
    "\n",
    "        self.pretrain_grad = tf.gradients(self.pretrain_loss, self.g_params)\n",
    "        self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))\n",
    "\n",
    "    def generate(self, session):\n",
    "        outputs = session.run(\n",
    "                [self.gen_x],\n",
    "                feed_dict={self.h0: np.random.normal(size=self.hidden_dim),\n",
    "                           self.samples: np.random.random(self.sequence_length)})\n",
    "        return outputs[0]\n",
    "\n",
    "    def train_g_step(self, session):\n",
    "        outputs = session.run(\n",
    "                [self.g_updates, self.reward_updates, self.g_loss,\n",
    "                 self.expected_reward, self.gen_x],\n",
    "                feed_dict={self.h0: np.random.normal(size=self.hidden_dim),\n",
    "                           self.samples: np.random.random(self.sequence_length)})\n",
    "        return outputs\n",
    "\n",
    "    def train_d_gen_step(self, session):\n",
    "        outputs = session.run(\n",
    "                [self.d_gen_updates, self.d_gen_loss],\n",
    "                feed_dict={self.h0: np.random.normal(size=self.hidden_dim),\n",
    "                           self.samples: np.random.random(self.sequence_length)})\n",
    "        return outputs\n",
    "\n",
    "    def train_d_real_step(self, session, x):\n",
    "        outputs = session.run([self.d_real_updates, self.d_real_loss],\n",
    "                              feed_dict={self.x: x})\n",
    "        return outputs\n",
    "\n",
    "    def pretrain_step(self, session, x):\n",
    "        outputs = session.run([self.pretrain_updates, self.pretrain_loss, self.g_predictions],\n",
    "                              feed_dict={self.x: x,\n",
    "                                         self.h0: np.random.normal(size=self.hidden_dim)})\n",
    "        return outputs\n",
    "\n",
    "    def init_matrix(self, shape):\n",
    "        return tf.compat.v1.random_normal(shape, stddev=0.1)\n",
    "\n",
    "    def init_vector(self, shape):\n",
    "        return tf.zeros(shape)\n",
    "\n",
    "    def create_recurrent_unit(self, params):\n",
    "        self.W_rec = tf.Variable(self.init_matrix([self.hidden_dim, self.emb_dim]))\n",
    "        params.append(self.W_rec)\n",
    "        def unit(x_t, h_tm1):\n",
    "            return h_tm1 + tf.reshape(tf.matmul(self.W_rec, tf.reshape(x_t, [self.emb_dim, 1])), [self.hidden_dim])\n",
    "        return unit\n",
    "\n",
    "    def create_output_unit(self, params, embeddings):\n",
    "        self.W_out = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
    "        self.b_out1 = tf.Variable(self.init_vector([self.emb_dim, 1]))\n",
    "        self.b_out2 = tf.Variable(self.init_vector([self.num_emb, 1]))\n",
    "        params.extend([self.W_out, self.b_out1, self.b_out2])\n",
    "        def unit(h_t):\n",
    "            logits = tf.reshape(\n",
    "                    self.b_out2 +\n",
    "                    tf.matmul(embeddings,\n",
    "                              tf.tanh(self.b_out1 +\n",
    "                                      tf.matmul(self.W_out, tf.reshape(h_t, [self.hidden_dim, 1])))),\n",
    "                    [1, self.num_emb])\n",
    "            return tf.reshape(tf.nn.softmax(logits), [self.num_emb])\n",
    "        return unit\n",
    "\n",
    "    def create_classifier_unit(self, params):\n",
    "        self.W_class = tf.Variable(self.init_matrix([1, self.hidden_dim]))\n",
    "        self.b_class = tf.Variable(self.init_vector([1]))\n",
    "        params.extend([self.W_class, self.b_class])\n",
    "        def unit(h_t):\n",
    "            return self.b_class + tf.matmul(self.W_class, tf.reshape(h_t, [self.hidden_dim, 1]))\n",
    "        return unit\n",
    "\n",
    "    def d_optimizer(self, *args, **kwargs):\n",
    "        return tf.compat.v1.train.GradientDescentOptimizer(*args, **kwargs)\n",
    "\n",
    "    def g_optimizer(self, *args, **kwargs):\n",
    "        return tf.compat.v1.train.GradientDescentOptimizer(*args, **kwargs)\n",
    "\n",
    "\n",
    "class GRU(RNN):\n",
    "\n",
    "    def create_recurrent_unit(self, params):\n",
    "        self.W_rx = tf.Variable(self.init_matrix([self.hidden_dim, self.emb_dim]))\n",
    "        self.W_zx = tf.Variable(self.init_matrix([self.hidden_dim, self.emb_dim]))\n",
    "        self.W_hx = tf.Variable(self.init_matrix([self.hidden_dim, self.emb_dim]))\n",
    "        self.U_rh = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        self.U_zh = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        self.U_hh = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        params.extend([\n",
    "            self.W_rx, self.W_zx, self.W_hx,\n",
    "            self.U_rh, self.U_zh, self.U_hh])\n",
    "\n",
    "        def unit(x_t, h_tm1):\n",
    "            x_t = tf.reshape(x_t, [self.emb_dim, 1])\n",
    "            h_tm1 = tf.reshape(h_tm1, [self.hidden_dim, 1])\n",
    "            r = tf.sigmoid(tf.matmul(self.W_rx, x_t) + tf.matmul(self.U_rh, h_tm1))\n",
    "            z = tf.sigmoid(tf.matmul(self.W_zx, x_t) + tf.matmul(self.U_zh, h_tm1))\n",
    "            h_tilda = tf.tanh(tf.matmul(self.W_hx, x_t) + tf.matmul(self.U_hh, r * h_tm1))\n",
    "            h_t = (1 - z) * h_tm1 + z * h_tilda\n",
    "            return tf.reshape(h_t, [self.hidden_dim])\n",
    "\n",
    "        return unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3OoePY7HplM"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uxj8jdWvHrdk"
   },
   "outputs": [],
   "source": [
    "def train_epoch(sess, trainable_model, num_iter,\n",
    "                proportion_supervised, g_steps, d_steps,\n",
    "                next_sequence, results, epoch, verify_sequence=None,\n",
    "                words=None,\n",
    "                proportion_generated=0.5):\n",
    "    \"\"\"Perform training for model.\n",
    "\n",
    "    sess: tensorflow session\n",
    "    trainable_model: the model\n",
    "    num_iter: number of iterations\n",
    "    proportion_supervised: what proportion of iterations should the generator\n",
    "        be trained in a supervised manner (rather than trained via discriminator)\n",
    "    g_steps: number of generator training steps per iteration\n",
    "    d_steps: number of discriminator t raining steps per iteration\n",
    "    next_sequence: function that returns a groundtruth sequence\n",
    "    verify_sequence: function that checks a generated sequence, returning True/False\n",
    "    words:  array of words (to map indices back to words)\n",
    "    proportion_generated: what proportion of steps for the discriminator\n",
    "        should be on artificially generated data\n",
    "\n",
    "    \"\"\"\n",
    "    supervised_g_losses = [0]  # we put in 0 to avoid empty slices\n",
    "    unsupervised_g_losses = [0]  # we put in 0 to avoid empty slices\n",
    "    d_losses = [0]\n",
    "    expected_rewards = [[0] * trainable_model.sequence_length]\n",
    "    supervised_correct_generation = [0]\n",
    "    unsupervised_correct_generation = [0]\n",
    "    supervised_gen_x = None\n",
    "    unsupervised_gen_x = None\n",
    "    print('running %d iterations with %d g steps and %d d steps' % (num_iter, g_steps, d_steps))\n",
    "    print('of the g steps, %.2f will be supervised' % proportion_supervised)\n",
    "    for it in range(num_iter):\n",
    "        for _ in range(g_steps):\n",
    "            if random.random() < proportion_supervised:\n",
    "                seq = next_sequence()\n",
    "                _, g_loss, g_pred = trainable_model.pretrain_step(sess, seq)\n",
    "                supervised_g_losses.append(g_loss)\n",
    "\n",
    "                supervised_gen_x = np.argmax(g_pred, axis=1)\n",
    "                if verify_sequence is not None:\n",
    "                    supervised_correct_generation.append(\n",
    "                        verify_sequence(supervised_gen_x))\n",
    "            else:\n",
    "                _, _, g_loss, expected_reward, unsupervised_gen_x = \\\n",
    "                    trainable_model.train_g_step(sess)\n",
    "                expected_rewards.append(expected_reward)\n",
    "                unsupervised_g_losses.append(g_loss)\n",
    "\n",
    "                if verify_sequence is not None:\n",
    "                    unsupervised_correct_generation.append(\n",
    "                        verify_sequence(unsupervised_gen_x))\n",
    "\n",
    "        for _ in range(d_steps):\n",
    "            if random.random() < proportion_generated:\n",
    "                seq = next_sequence()\n",
    "                _, d_loss = trainable_model.train_d_real_step(sess, seq)\n",
    "            else:\n",
    "                _, d_loss = trainable_model.train_d_gen_step(sess)\n",
    "            d_losses.append(d_loss)\n",
    "    \n",
    "\n",
    "    results['d_losses'][epoch] = np.mean(d_losses)\n",
    "    results['supervised_g_losses'][epoch] = np.mean(supervised_g_losses)\n",
    "    results['unsupervised_g_losses'][epoch] = np.mean(unsupervised_g_losses)\n",
    "    results['supervised_generations'][epoch] = [words[x] if words else x for x in supervised_gen_x] if supervised_gen_x is not None else None\n",
    "    results['unsupervised_generations'][epoch] = [words[x] if words else x for x in unsupervised_gen_x] if unsupervised_gen_x is not None else None\n",
    "    results['rewards'][epoch] = np.mean(expected_rewards, axis=0)\n",
    "    print('epoch statistics:')\n",
    "    print('>>>> discriminator loss:', results['d_losses'][epoch])\n",
    "    \n",
    "    print('>>>> generator loss:', results['supervised_g_losses'][epoch], results['unsupervised_g_losses'][epoch])\n",
    "\n",
    "    if verify_sequence is not None:\n",
    "        print('>>>> correct generations (supervised, unsupervised):', np.mean(supervised_correct_generation), np.mean(unsupervised_correct_generation))\n",
    "    print('>>>> sampled generations (supervised, unsupervised):',)\n",
    "    print(results['supervised_generations'][epoch],)\n",
    "    print(results['unsupervised_generations'][epoch])\n",
    "    print('>>>> expected rewards:', results['rewards'][epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMib4ftTHx8g"
   },
   "source": [
    "## SeqGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kACD4QhWH0M3"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "EMB_DIM = 20\n",
    "HIDDEN_DIM = 25\n",
    "SEQ_LENGTH = 10\n",
    "START_TOKEN = 0\n",
    "\n",
    "EPOCH_ITER = 1000\n",
    "CURRICULUM_RATE = 0.02  # how quickly to move from supervised training to unsupervised\n",
    "TRAIN_ITER = 100000  # generator/discriminator alternating\n",
    "D_STEPS = 3  # how many times to train the discriminator per generator step\n",
    "SEED = 88\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return [c for c in ' '.join(s.split())]\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    if not os.path.isfile('sessions.pkl') or not os.path.isfile('all_tracks.pkl'):\n",
    "        token_stream, all_tracks = get_sessions(100000)\n",
    "        token_stream.to_pickle(\"sessions.pkl\")\n",
    "        output = open('all_tracks.pkl', 'wb')\n",
    "        pickle.dump(all_tracks, output)\n",
    "    else:\n",
    "        print(\"Sessions exist, using sessions.pkl and all_tracks.pkl\")\n",
    "        token_stream = pd.read_pickle(\"sessions.pkl\")\n",
    "        all_tracks = pd.read_pickle(\"all_tracks.pkl\")\n",
    "    return token_stream, all_tracks\n",
    "\n",
    "\n",
    "class BookGRU(GRU):\n",
    "\n",
    "    def d_optimizer(self, *args, **kwargs):\n",
    "        return tf.compat.v1.train.AdamOptimizer()  # ignore learning rate\n",
    "\n",
    "    def g_optimizer(self, *args, **kwargs):\n",
    "        return tf.compat.v1.train.AdamOptimizer()  # ignore learning rate\n",
    "\n",
    "\n",
    "def get_trainable_model(num_emb):\n",
    "    return BookGRU(\n",
    "        num_emb, EMB_DIM, HIDDEN_DIM,\n",
    "        SEQ_LENGTH, START_TOKEN)\n",
    "\n",
    "\n",
    "def get_random_sequence(token_stream):\n",
    "    \"\"\"Returns random subsequence.\"\"\"\n",
    "\n",
    "\n",
    "    while True:\n",
    "        row_idx = random.randint(0, len(token_stream)-1)\n",
    "        if len(token_stream[row_idx]) >= SEQ_LENGTH:\n",
    "            break\n",
    "\n",
    "    start_idx = random.randint(0, len(token_stream[row_idx]) - SEQ_LENGTH)\n",
    "\n",
    "    return token_stream[row_idx][start_idx:start_idx + SEQ_LENGTH]\n",
    "\n",
    "\n",
    "def verify_sequence(three_grams, seq):\n",
    "    \"\"\"Not a true verification; only checks 3-grams.\"\"\"\n",
    "    for i in range(len(seq) - 3):\n",
    "        if tuple(seq[i:i + 3]) not in three_grams:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def init_dict():\n",
    "    results = {}\n",
    "    results['d_losses'] = {}\n",
    "    results['supervised_g_losses'] = {}\n",
    "    results['unsupervised_g_losses'] = {}\n",
    "    results['supervised_generations'] = {}\n",
    "    results['unsupervised_generations'] = {}\n",
    "    results['rewards'] = {}\n",
    "    return results\n",
    "\n",
    "\n",
    "def run():\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    token_stream, all_tracks = get_data() # Read in data & create sessions\n",
    "    results = init_dict() # initialise results dictionary\n",
    "    \n",
    "    # create words from all track_ids\n",
    "    track_keys = []\n",
    "    for key in all_tracks.keys():\n",
    "        track_keys.append(key)\n",
    "    words = track_keys\n",
    "\n",
    "    # create index to word dicttionary for track_ids\n",
    "    idx2word = {}\n",
    "    for i in range(len(all_tracks)):\n",
    "        idx2word[i] = all_tracks.get(i)['track_name']\n",
    "    \n",
    "    num_words = len(words)\n",
    "    three_grams = {}\n",
    "    count=0\n",
    "    sec_count=0\n",
    "\n",
    "    # create dictionary of 3-gram verification values\n",
    "    for idx, row in token_stream.iteritems():\n",
    "        sec_count += len(row)\n",
    "        if len(row) > 3 :\n",
    "            for i in range(len(row) - 3):\n",
    "                three_grams[tuple(w for w in row[i:i + 3])] = True\n",
    "        else : count += len(row)\n",
    "    \n",
    "    \n",
    "    # print(\"Less than |3| = \", count)\n",
    "    # print(\"Total count = \", sec_count)\n",
    "    # print('num words', num_words)\n",
    "    # print('stream length', len(token_stream))\n",
    "    # print('distinct 3-grams', len(three_grams))\n",
    "\n",
    "    trainable_model = get_trainable_model(num_words)\n",
    "    sess = tf.compat.v1.Session()\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    print('Training...')\n",
    "    for epoch in range(TRAIN_ITER // EPOCH_ITER):\n",
    "        print('epoch', epoch)\n",
    "        proportion_supervised = max(0.0, 1.0 - CURRICULUM_RATE * epoch)\n",
    "        train_epoch(\n",
    "            sess, trainable_model, EPOCH_ITER,\n",
    "            proportion_supervised=proportion_supervised,\n",
    "            g_steps=1, d_steps=D_STEPS,\n",
    "            next_sequence=lambda: get_random_sequence(token_stream),\n",
    "            verify_sequence=lambda seq: verify_sequence(three_grams, seq),\n",
    "            words=words, results=results, epoch=epoch)\n",
    "\n",
    "    print(\"Time taken: \", time.time()-start_time)\n",
    "\n",
    "    # Save results for each value of D_STEPS\n",
    "    with open('results_'+str(D_STEPS)+'.pkl', 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return token_stream, all_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2rqwyrW-zKI"
   },
   "source": [
    "## Run SeqGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dPrs_jXcQEj0"
   },
   "outputs": [],
   "source": [
    "token_stream, all_tracks = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyqCDjhF-5Cn"
   },
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6rPoo1AFQIFs"
   },
   "outputs": [],
   "source": [
    "def plotCurve(loss, legend, title):\n",
    "    plt.clf()\n",
    "    plt.plot(loss)\n",
    "    plt.title('Learning Curve')\n",
    "    plt.ylabel('NLL')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(legend, loc='upper right')\n",
    "    plt.savefig(title+\".png\", bbox_inches=\"tight\", dpi=100)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plotBLEU(generations, title, legend):    \n",
    "  plt.clf() \n",
    "  plt.plot(generations)\n",
    "  plt.title(title)\n",
    "  plt.ylabel('BLEU')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(legend, loc='upper right')\n",
    "  plt.savefig(title+\".png\", bbox_inches=\"tight\", dpi=100)\n",
    "  plt.show()\n",
    "  \n",
    "\n",
    "def plotSessions(results, label, filename):\n",
    "  plt.clf()\n",
    "  plt.ylabel(\"Number of Sessions\")\n",
    "  plt.xlabel(\"Session Length (minutes)\")\n",
    "\n",
    "  cmap = cm.get_cmap('viridis')\n",
    "  bins = np.linspace(0, 50, 10)\n",
    "  \n",
    "  plt.style.use('seaborn-deep')\n",
    "  plt.hist([results[5], results[35]], bins, label=label)\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.savefig(filename, dpi=100)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGU51vOQCsSO"
   },
   "source": [
    "## Plot Session Lengths for varios values of Δt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk0lgng8C1mK"
   },
   "outputs": [],
   "source": [
    "plotSessions([results[5], results[35]], \n",
    "             ['\\u0394t = 5 minutes', '\\u0394t = 35 minutes'], \n",
    "             \"delta5_35_histogram.png\")\n",
    "\n",
    "plotSessions([results[10], results[20], results[30]], \n",
    "             ['\\u0394t = 10 minutes', '\\u0394t = 20 minutes', '\\u0394t = 30 minutes'], \n",
    "             \"delta10_20_30_histogram.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kp_Mvigg_abm"
   },
   "source": [
    "## Plot Learning Curve for various values of D_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIbVRIkoZCY6"
   },
   "outputs": [],
   "source": [
    "\n",
    "for D_STEPS in range(1,5):\n",
    "    with open('results_'+str(D_STEPS)+'.pkl', 'rb') as handle:\n",
    "        res = pickle.load(handle)\n",
    "    x = []    \n",
    "    for r in range(100):\n",
    "        x.append(res['unsupervised_g_losses'][r])\n",
    "    print(\"Loss for D_STEPS = {}: {}\".format(D_STEPS, res['unsupervised_g_losses'][99]))\n",
    "        \n",
    "    plotCurve(x, ['D_STEPS = '+str(D_STEPS)], \"CMON_\"+str(D_STEPS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuXB87d7_kdq"
   },
   "source": [
    "## Plot BLEU Score for each generation for various values of D_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cy9o2NPRkxfg"
   },
   "outputs": [],
   "source": [
    "\n",
    "for D_STEPS in range(1,5):\n",
    "  with open('results_'+str(D_STEPS)+'.pkl', 'rb') as handle:\n",
    "      results = pickle.load(handle)\n",
    "  x = []\n",
    "  for epoch in range(100):\n",
    "    if results['unsupervised_generations'][epoch] is not None:\n",
    "      x.append(sentence_bleu(token_stream, results['unsupervised_generations'][epoch], smoothing_function=SmoothingFunction().method1))\n",
    "  \n",
    "  plotBLEU(x, \"BLEU_\"+str(D_STEPS), \"D_STEP = \"+str(D_STEPS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Inp7AsGu_-lu"
   },
   "source": [
    "## Peer versus BLEU Score Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7jubv3n45B-"
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "with open('results_3.pkl', 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "  # print(epoch, type(results['unsupervised_generations'][epoch]))\n",
    "    for epoch in range(96, 100):\n",
    "        print(\"Epoch {}: {}\".format(epoch, results['unsupervised_generations'][epoch]))\n",
    "        for tr in results['unsupervised_generations'][epoch]:\n",
    "            print(\"{} - {}\".format(all_tracks[tr]['track_name'], all_tracks[tr]['artist']))\n",
    "        print(\"{:2f}\".format(sentence_bleu(token_stream, results['unsupervised_generations'][epoch], smoothing_function=SmoothingFunction().method4)))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
